{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe12cefb-43c4-45d1-b0cf-1012da72473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb, os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e6c2d-801d-4041-9d13-92c2b3ff3e88",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Datasets Library\n",
    "> We will prepare the data for the transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5362bf-f492-4601-88c8-3ab6fdb8357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Features, ClassLabel, Value, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "385b3e03-7db0-4411-8b5f-09a2c9d939e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapecape\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/capecape/aws_demo/runs/mda6pzd7\" target=\"_blank\">autumn-voice-18</a></strong> to <a href=\"https://wandb.ai/capecape/aws_demo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/capecape/aws_demo/runs/mda6pzd7?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f8c22198850>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"aws_demo\", job_type=\"get_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f503ac3-ff35-4283-83ac-9a3a584d9e2d",
   "metadata": {},
   "source": [
    "we can grab the preprocessed dataset direcly from wandb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0d747fd-f840-4f18-89e1-b9eb7643b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path  = wandb.use_artifact(\"capecape/aws_demo/splitted_dataset:latest\").download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5a5146d-9476-446a-9163-c2475cc126df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./artifacts/splitted_dataset:v0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa52c7f5-86d4-4c51-a0d7-68d1a891de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"negative\", \"positive\"]\n",
    "stock_features = Features({'Text': Value('string'), \n",
    "                           'labels': ClassLabel(names=labels)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c23475e-c242-498d-af53-f8d9f92e6e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d8dfb5a2e7af0ec3\n",
      "Reusing dataset csv (/home/paperspace/.cache/huggingface/datasets/csv/default-d8dfb5a2e7af0ec3/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c33c0ce11b475d81adeb3bbb07b81d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files={\"train\": os.path.join(dataset_path,\"train.csv\"), \n",
    "                                          \"test\": os.path.join(dataset_path, \"test.csv\")}, \n",
    "                       delimiter=',', \n",
    "                       features=stock_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9653380a-2e41-4886-afb4-3f66d7a01e13",
   "metadata": {},
   "source": [
    "we get a `DatasetDict` object containing our split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b254b448-d2dd-4fa2-8cf9-65b07303de24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Text', 'labels'],\n",
       "        num_rows: 5212\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Text', 'labels'],\n",
       "        num_rows: 579\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6fe0c3-05e5-449f-8eba-728a9a5fe255",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A Simple bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b28158e0-dbbe-4080-b303-d8be4715dee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98a3ce1b-028c-4ab7-89cc-e36388c4d525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/paperspace/.cache/huggingface/datasets/csv/default-d8dfb5a2e7af0ec3/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-055f664b758555b4.arrow\n",
      "Loading cached processed dataset at /home/paperspace/.cache/huggingface/datasets/csv/default-d8dfb5a2e7af0ec3/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-946972fb275e0871.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='max_length')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"Text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b89df03-e7bc-46e3-a809-8d14f69937d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5212\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 579\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeddcaad-38f6-4067-a15f-8ee426a0bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_training_args = {\n",
    "    'per_device_train_batch_size': 64,\n",
    "    'per_device_eval_batch_size': 64,\n",
    "    'num_train_epochs': 2,\n",
    "    'learning_rate': 2e-5,\n",
    "    'evaluation_strategy': 'epoch',\n",
    "    'save_strategy': 'epoch',\n",
    "    'save_total_limit': 2,\n",
    "    'logging_strategy': 'steps',\n",
    "    'logging_first_step': True,\n",
    "    'logging_steps': 5,\n",
    "    'report_to': 'wandb',\n",
    "    'fp16':True,\n",
    "    'dataloader_num_workers':6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f0162ba-f30d-4639-a4b2-5888aa8edd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_metric = load_metric(\"recall\")\n",
    "f1_metric = load_metric('f1')\n",
    "accuracy_metric = load_metric('accuracy')\n",
    "precision_metric = load_metric('precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5646f1fa-3b8e-4f10-9250-dfb6ee8a1fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.sdk.integration_utils.data_logging import ValidationDataLogger\n",
    "\n",
    "validation_inputs = tokenized_datasets['test'].remove_columns(['labels', 'attention_mask', 'input_ids', 'token_type_ids'])\n",
    "validation_targets = [tokenized_datasets['test'].features['labels'].int2str(x) for x in dataset['test']['labels']]\n",
    "\n",
    "validation_logger = ValidationDataLogger(inputs = validation_inputs[:],targets = validation_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b98c50b1-47bf-4a07-8544-8ead16eeea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"Get a bunch of metrics and log predictions to wandb\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average='macro')['recall']\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='macro')['f1']\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions,references=labels)['accuracy']\n",
    "    precision = precision_metric.compute(predictions=predictions,references=labels,average='macro')['precision']\n",
    "    \n",
    "    # convert predictions from class (0, 1) to label (Negative, Positive)\n",
    "    prediction_labels = [tokenized_datasets['test'].features['labels'].int2str(x.item()) for x in predictions]\n",
    "    \n",
    "    # log predictions\n",
    "    validation_logger.log_predictions(prediction_labels)\n",
    "\n",
    "    return {\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df0bf015-89ca-4675-869a-eb7a68b9d834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainer(model, output_dir, tokenizer, data_collator, training_args, train, test):\n",
    "    \"Prepare the hf Trainer\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        **training_args\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=train,\n",
    "        eval_dataset=test,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065f5d60-3a38-4795-8d54-df7a9e646403",
   "metadata": {},
   "source": [
    "Let's log predictions at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8754f8fc-a0a2-4f91-8423-e6783ec3e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_args=default_training_args, model_name=\"bert-base-cased\"):\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    trainer = get_trainer(\n",
    "        output_dir=f'training_dir',\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        training_args=train_args,\n",
    "        train=tokenized_datasets['train'],\n",
    "        test=tokenized_datasets[\"test\"])\n",
    "    \n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a09fae9-3140-4131-9ce9-2b3e4ab5be76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running training *****\n",
      "  Num examples = 5212\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 164\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='164' max='164' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [164/164 02:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.597700</td>\n",
       "      <td>0.587250</td>\n",
       "      <td>0.666808</td>\n",
       "      <td>0.666051</td>\n",
       "      <td>0.683938</td>\n",
       "      <td>0.665405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.461700</td>\n",
       "      <td>0.488913</td>\n",
       "      <td>0.741067</td>\n",
       "      <td>0.745429</td>\n",
       "      <td>0.765112</td>\n",
       "      <td>0.752386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_dir/checkpoint-82\n",
      "Configuration saved in training_dir/checkpoint-82/config.json\n",
      "Model weights saved in training_dir/checkpoint-82/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-82/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-82/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to training_dir/checkpoint-164\n",
      "Configuration saved in training_dir/checkpoint-164/config.json\n",
      "Model weights saved in training_dir/checkpoint-164/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-164/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-164/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b3a1b9d-3095-4987-bd43-ccea2cd0f760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5553... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.04MB of 0.04MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁█</td></tr><tr><td>eval/f1</td><td>▁█</td></tr><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/precision</td><td>▁█</td></tr><tr><td>eval/recall</td><td>▁█</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▇█▆▆▅█▆▆▆▇▇▆▆▆▆▆▅▄▄▃▃▃▂▃▃▂▂▁▃▂▁▃▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.76511</td></tr><tr><td>eval/f1</td><td>0.74543</td></tr><tr><td>eval/loss</td><td>0.48891</td></tr><tr><td>eval/precision</td><td>0.75239</td></tr><tr><td>eval/recall</td><td>0.74107</td></tr><tr><td>eval/runtime</td><td>3.0854</td></tr><tr><td>eval/samples_per_second</td><td>187.656</td></tr><tr><td>eval/steps_per_second</td><td>3.241</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>164</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4617</td></tr><tr><td>train/total_flos</td><td>2742669641072640.0</td></tr><tr><td>train/train_loss</td><td>0.56248</td></tr><tr><td>train/train_runtime</td><td>136.9626</td></tr><tr><td>train/train_samples_per_second</td><td>76.108</td></tr><tr><td>train/train_steps_per_second</td><td>1.197</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">autumn-voice-18</strong>: <a href=\"https://wandb.ai/capecape/aws_demo/runs/mda6pzd7\" target=\"_blank\">https://wandb.ai/capecape/aws_demo/runs/mda6pzd7</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220131_151814-mda6pzd7/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae87d3-da43-496c-901e-3373bd3f4ac0",
   "metadata": {},
   "source": [
    "## Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6db8e3ed-94ef-4e95-bf12-11198e49b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df4a6d77-8834-4bb0-ba0f-ac50349c8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37696c9b-22a4-4997-a8dc-c794f2e4d494",
   "metadata": {},
   "source": [
    "For `bayes`ian Sweeps,\n",
    "you also need to tell us a bit about your `metric`.\n",
    "We need to know its `name`, so we can find it in the model outputs\n",
    "and we need to know whether your `goal` is to `minimize` it\n",
    "(e.g. if it's the squared error)\n",
    "or to `maximize` it\n",
    "(e.g. if it's the accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4ecd544-9f5c-43db-ab7e-ed3cbd275e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = {\n",
    "    'name': 'eval/loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d37440-0590-4af7-8067-76236f994ff3",
   "metadata": {},
   "source": [
    "Once you've picked a `method` to try out new values of the hyperparameters,\n",
    "you need to define what those `parameters` are.\n",
    "\n",
    "Most of the time, this step is straightforward:\n",
    "you just give the `parameter` a name\n",
    "and specify a list of legal `values`\n",
    "of the parameter.\n",
    "\n",
    "For example, when we choose the `optimizer` for our network,\n",
    "there's only a finite number of options.\n",
    "Here we stick with the two most popular choices, `adam` and `sgd`.\n",
    "Even for hyperparameters that have potentially infinite options,\n",
    "it usually only makes sense to try out\n",
    "a few select `values`,\n",
    "as we do here with the hidden `layer_size` and `dropout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38c62919-958e-42c6-ae0e-6f4943056aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict = {\n",
    "    'learning_rate': {\n",
    "        # a flat distribution between 0 and 0.1\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0,\n",
    "        'max': 0.001\n",
    "      },\n",
    "    'batch_size': {\n",
    "        # integers between 32 and 256\n",
    "        # with evenly-distributed logarithms \n",
    "        'distribution': 'q_log_uniform',\n",
    "        'q': 1,\n",
    "        'min': math.log(16),\n",
    "        'max': math.log(64),\n",
    "      },\n",
    "    'epochs': {\n",
    "        \"values\": [4,6,8,10]\n",
    "    }\n",
    "}\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ccc2634-6d40-44f7-abab-aa0c3868439d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 60gv5p2v\n",
      "Sweep URL: https://wandb.ai/capecape/aws_demo/sweeps/60gv5p2v\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"aws_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71908b79-2e04-4f1d-a77c-6bb1be661607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'bayes',\n",
       " 'metric': {'name': 'eval/loss', 'goal': 'minimize'},\n",
       " 'parameters': {'learning_rate': {'distribution': 'uniform',\n",
       "   'min': 0,\n",
       "   'max': 0.001},\n",
       "  'batch_size': {'distribution': 'q_log_uniform',\n",
       "   'q': 1,\n",
       "   'min': 2.772588722239781,\n",
       "   'max': 4.1588830833596715},\n",
       "  'epochs': {'values': [4, 6, 8, 10]}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sweep_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8504487d-84d2-41d5-a7e1-ebb24ca6a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sweep(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "        \n",
    "        default_training_args[\"learning_rate\"] = config.learning_rate\n",
    "        default_training_args['per_device_train_batch_size'] = config.batch_size\n",
    "        default_training_args['per_device_eval_batch_size'] = config.batch_size\n",
    "        default_training_args[\"num_train_epochs\"] = config.epochs\n",
    "        \n",
    "        train(default_training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f2c0991-5bc0-47d6-8e79-136cb3bc2c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9zv9oary with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005886648905335285\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/capecape/aws_demo/runs/9zv9oary\" target=\"_blank\">proud-sweep-1</a></strong> to <a href=\"https://wandb.ai/capecape/aws_demo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/capecape/aws_demo/sweeps/60gv5p2v\" target=\"_blank\">https://wandb.ai/capecape/aws_demo/sweeps/60gv5p2v</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/paperspace/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /home/paperspace/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running training *****\n",
      "  Num examples = 5212\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 42\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 42\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1250\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 11:10, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.652900</td>\n",
       "      <td>0.682121</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.689600</td>\n",
       "      <td>0.712495</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.275344</td>\n",
       "      <td>0.379965</td>\n",
       "      <td>0.189983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.689700</td>\n",
       "      <td>0.669273</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.730300</td>\n",
       "      <td>0.665162</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.683000</td>\n",
       "      <td>0.667391</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>0.664898</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.715600</td>\n",
       "      <td>0.664385</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.670746</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.654000</td>\n",
       "      <td>0.666067</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.674800</td>\n",
       "      <td>0.664293</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 42\n",
      "Saving model checkpoint to training_dir/checkpoint-125\n",
      "Configuration saved in training_dir/checkpoint-125/config.json\n",
      "Model weights saved in training_dir/checkpoint-125/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-125/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-125/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-82] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 42\n",
      "Saving model checkpoint to training_dir/checkpoint-250\n",
      "Configuration saved in training_dir/checkpoint-250/config.json\n",
      "Model weights saved in training_dir/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-164] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 42\n",
      "Saving model checkpoint to training_dir/checkpoint-375\n",
      "Configuration saved in training_dir/checkpoint-375/config.json\n",
      "Model weights saved in training_dir/checkpoint-375/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-375/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-375/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-125] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 42\n",
      "Saving model checkpoint to training_dir/checkpoint-500\n",
      "Configuration saved in training_dir/checkpoint-500/config.json\n",
      "Model weights saved in training_dir/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-250] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 42\n",
      "Saving model checkpoint to training_dir/checkpoint-625\n",
      "Configuration saved in training_dir/checkpoint-625/config.json\n",
      "Model weights saved in training_dir/checkpoint-625/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-625/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-625/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-375] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 42\n",
      "Saving model checkpoint to training_dir/checkpoint-750\n",
      "Configuration saved in training_dir/checkpoint-750/config.json\n",
      "Model weights saved in training_dir/checkpoint-750/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-750/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 42\n",
      "Saving model checkpoint to training_dir/checkpoint-875\n",
      "Configuration saved in training_dir/checkpoint-875/config.json\n",
      "Model weights saved in training_dir/checkpoint-875/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-875/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-875/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-625] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 42\n",
      "Saving model checkpoint to training_dir/checkpoint-1000\n",
      "Configuration saved in training_dir/checkpoint-1000/config.json\n",
      "Model weights saved in training_dir/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-750] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 42\n",
      "Saving model checkpoint to training_dir/checkpoint-1125\n",
      "Configuration saved in training_dir/checkpoint-1125/config.json\n",
      "Model weights saved in training_dir/checkpoint-1125/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-1125/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-1125/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-875] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 42\n",
      "Saving model checkpoint to training_dir/checkpoint-1250\n",
      "Configuration saved in training_dir/checkpoint-1250/config.json\n",
      "Model weights saved in training_dir/checkpoint-1250/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-1250/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-1250/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-1000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5893... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.21MB of 0.21MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>█▁████████</td></tr><tr><td>eval/f1</td><td>█▁████████</td></tr><tr><td>eval/loss</td><td>▄█▂▁▁▁▁▂▁▁</td></tr><tr><td>eval/precision</td><td>█▁████████</td></tr><tr><td>eval/recall</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▁▁▆▁█▁▂▁▁</td></tr><tr><td>eval/samples_per_second</td><td>█▇█▃█▁█▇▇▇</td></tr><tr><td>eval/steps_per_second</td><td>█▇█▃█▁█▇▇▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▂▄▃▂▃▃▃▄▂▄▂▃▃▃▃▂▃▃▃▄▁▂▃▃▂▂▂▃▃▁▄▂▃▂▂▁▂▂▄</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.62003</td></tr><tr><td>eval/f1</td><td>0.38273</td></tr><tr><td>eval/loss</td><td>0.66429</td></tr><tr><td>eval/precision</td><td>0.31002</td></tr><tr><td>eval/recall</td><td>0.5</td></tr><tr><td>eval/runtime</td><td>3.0552</td></tr><tr><td>eval/samples_per_second</td><td>189.512</td></tr><tr><td>eval/steps_per_second</td><td>4.582</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/global_step</td><td>1250</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6748</td></tr><tr><td>train/total_flos</td><td>1.37133482053632e+16</td></tr><tr><td>train/train_loss</td><td>0.67001</td></tr><tr><td>train/train_runtime</td><td>671.3148</td></tr><tr><td>train/train_samples_per_second</td><td>77.639</td></tr><tr><td>train/train_steps_per_second</td><td>1.862</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 10 media file(s), 10 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">proud-sweep-1</strong>: <a href=\"https://wandb.ai/capecape/aws_demo/runs/9zv9oary\" target=\"_blank\">https://wandb.ai/capecape/aws_demo/runs/9zv9oary</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220131_152112-9zv9oary/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n7elyuoi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.905438312333176e-05\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/capecape/aws_demo/runs/n7elyuoi\" target=\"_blank\">deft-sweep-2</a></strong> to <a href=\"https://wandb.ai/capecape/aws_demo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/capecape/aws_demo/sweeps/60gv5p2v\" target=\"_blank\">https://wandb.ai/capecape/aws_demo/sweeps/60gv5p2v</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/paperspace/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /home/paperspace/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running training *****\n",
      "  Num examples = 5212\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 60\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 60\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 348\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='348' max='348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [348/348 04:21, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.443300</td>\n",
       "      <td>0.466930</td>\n",
       "      <td>0.776557</td>\n",
       "      <td>0.765437</td>\n",
       "      <td>0.772021</td>\n",
       "      <td>0.762992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.327800</td>\n",
       "      <td>0.411963</td>\n",
       "      <td>0.790770</td>\n",
       "      <td>0.798979</td>\n",
       "      <td>0.816926</td>\n",
       "      <td>0.814167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.113800</td>\n",
       "      <td>0.520385</td>\n",
       "      <td>0.802646</td>\n",
       "      <td>0.810784</td>\n",
       "      <td>0.827288</td>\n",
       "      <td>0.825175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.066600</td>\n",
       "      <td>0.607803</td>\n",
       "      <td>0.818410</td>\n",
       "      <td>0.823081</td>\n",
       "      <td>0.835924</td>\n",
       "      <td>0.829473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 60\n",
      "Saving model checkpoint to training_dir/checkpoint-87\n",
      "Configuration saved in training_dir/checkpoint-87/config.json\n",
      "Model weights saved in training_dir/checkpoint-87/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-87/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-87/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-1125] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 60\n",
      "Saving model checkpoint to training_dir/checkpoint-174\n",
      "Configuration saved in training_dir/checkpoint-174/config.json\n",
      "Model weights saved in training_dir/checkpoint-174/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-174/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-174/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-1250] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 60\n",
      "Saving model checkpoint to training_dir/checkpoint-261\n",
      "Configuration saved in training_dir/checkpoint-261/config.json\n",
      "Model weights saved in training_dir/checkpoint-261/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-261/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-261/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-87] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 60\n",
      "Saving model checkpoint to training_dir/checkpoint-348\n",
      "Configuration saved in training_dir/checkpoint-348/config.json\n",
      "Model weights saved in training_dir/checkpoint-348/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-348/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-348/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-174] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6802... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.09MB of 0.09MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▆▇█</td></tr><tr><td>eval/f1</td><td>▁▅▇█</td></tr><tr><td>eval/loss</td><td>▃▁▅█</td></tr><tr><td>eval/precision</td><td>▁▆██</td></tr><tr><td>eval/recall</td><td>▁▃▅█</td></tr><tr><td>eval/runtime</td><td>▄▁█▃</td></tr><tr><td>eval/samples_per_second</td><td>▅█▁▆</td></tr><tr><td>eval/steps_per_second</td><td>▅█▁▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▇█▇▇▇▆▆▆▅▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▃▂▂▁▁▁▁▁▁▂▁▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.83592</td></tr><tr><td>eval/f1</td><td>0.82308</td></tr><tr><td>eval/loss</td><td>0.6078</td></tr><tr><td>eval/precision</td><td>0.82947</td></tr><tr><td>eval/recall</td><td>0.81841</td></tr><tr><td>eval/runtime</td><td>3.1018</td></tr><tr><td>eval/samples_per_second</td><td>186.666</td></tr><tr><td>eval/steps_per_second</td><td>3.224</td></tr><tr><td>train/epoch</td><td>4.0</td></tr><tr><td>train/global_step</td><td>348</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0666</td></tr><tr><td>train/total_flos</td><td>5485339282145280.0</td></tr><tr><td>train/train_loss</td><td>0.29841</td></tr><tr><td>train/train_runtime</td><td>262.6821</td></tr><tr><td>train/train_samples_per_second</td><td>79.366</td></tr><tr><td>train/train_steps_per_second</td><td>1.325</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 4 media file(s), 4 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">deft-sweep-2</strong>: <a href=\"https://wandb.ai/capecape/aws_demo/runs/n7elyuoi\" target=\"_blank\">https://wandb.ai/capecape/aws_demo/runs/n7elyuoi</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220131_153237-n7elyuoi/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k24yo1h4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 22\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0006580379315536693\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/capecape/aws_demo/runs/k24yo1h4\" target=\"_blank\">silvery-sweep-3</a></strong> to <a href=\"https://wandb.ai/capecape/aws_demo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/capecape/aws_demo/sweeps/60gv5p2v\" target=\"_blank\">https://wandb.ai/capecape/aws_demo/sweeps/60gv5p2v</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/paperspace/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /home/paperspace/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running training *****\n",
      "  Num examples = 5212\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 22\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 22\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2370\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2370' max='2370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2370/2370 12:17, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>0.712924</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.655500</td>\n",
       "      <td>0.711437</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.275344</td>\n",
       "      <td>0.379965</td>\n",
       "      <td>0.189983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.639700</td>\n",
       "      <td>0.671210</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.713100</td>\n",
       "      <td>0.719186</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.275344</td>\n",
       "      <td>0.379965</td>\n",
       "      <td>0.189983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.665253</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.701800</td>\n",
       "      <td>0.686629</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.651300</td>\n",
       "      <td>0.666775</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.722400</td>\n",
       "      <td>0.665412</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.641200</td>\n",
       "      <td>0.664025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.668500</td>\n",
       "      <td>0.664177</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 22\n",
      "Saving model checkpoint to training_dir/checkpoint-237\n",
      "Configuration saved in training_dir/checkpoint-237/config.json\n",
      "Model weights saved in training_dir/checkpoint-237/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-237/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-237/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-261] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 22\n",
      "Saving model checkpoint to training_dir/checkpoint-474\n",
      "Configuration saved in training_dir/checkpoint-474/config.json\n",
      "Model weights saved in training_dir/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-474/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-348] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 22\n",
      "Saving model checkpoint to training_dir/checkpoint-711\n",
      "Configuration saved in training_dir/checkpoint-711/config.json\n",
      "Model weights saved in training_dir/checkpoint-711/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-711/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-711/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-237] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 22\n",
      "Saving model checkpoint to training_dir/checkpoint-948\n",
      "Configuration saved in training_dir/checkpoint-948/config.json\n",
      "Model weights saved in training_dir/checkpoint-948/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-948/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-948/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-474] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 22\n",
      "Saving model checkpoint to training_dir/checkpoint-1185\n",
      "Configuration saved in training_dir/checkpoint-1185/config.json\n",
      "Model weights saved in training_dir/checkpoint-1185/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-1185/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-1185/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-711] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 22\n",
      "Saving model checkpoint to training_dir/checkpoint-1422\n",
      "Configuration saved in training_dir/checkpoint-1422/config.json\n",
      "Model weights saved in training_dir/checkpoint-1422/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-1422/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-1422/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-948] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 22\n",
      "Saving model checkpoint to training_dir/checkpoint-1659\n",
      "Configuration saved in training_dir/checkpoint-1659/config.json\n",
      "Model weights saved in training_dir/checkpoint-1659/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-1659/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-1659/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-1185] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 22\n",
      "Saving model checkpoint to training_dir/checkpoint-1896\n",
      "Configuration saved in training_dir/checkpoint-1896/config.json\n",
      "Model weights saved in training_dir/checkpoint-1896/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-1896/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-1896/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-1422] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 22\n",
      "Saving model checkpoint to training_dir/checkpoint-2133\n",
      "Configuration saved in training_dir/checkpoint-2133/config.json\n",
      "Model weights saved in training_dir/checkpoint-2133/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-2133/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-2133/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-1659] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 22\n",
      "Saving model checkpoint to training_dir/checkpoint-2370\n",
      "Configuration saved in training_dir/checkpoint-2370/config.json\n",
      "Model weights saved in training_dir/checkpoint-2370/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-2370/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-2370/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-1896] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7187... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.21MB of 0.21MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>█▁█▁██████</td></tr><tr><td>eval/f1</td><td>█▁█▁██████</td></tr><tr><td>eval/loss</td><td>▇▇▂█▁▄▁▁▁▁</td></tr><tr><td>eval/precision</td><td>█▁█▁██████</td></tr><tr><td>eval/recall</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▃█▇▁▁▄▂▂▇█</td></tr><tr><td>eval/samples_per_second</td><td>▆▁▂██▄▇▇▂▁</td></tr><tr><td>eval/steps_per_second</td><td>▆▁▂██▄▇▇▂▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▄▆▄▅▆▆▅▄▃▃▃▄▃█▄▄▂▄▄▄▇▅▂▆▃▆▃▅▄▄▃▄▄▅▂▂▂▁▆▄</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.62003</td></tr><tr><td>eval/f1</td><td>0.38273</td></tr><tr><td>eval/loss</td><td>0.66418</td></tr><tr><td>eval/precision</td><td>0.31002</td></tr><tr><td>eval/recall</td><td>0.5</td></tr><tr><td>eval/runtime</td><td>3.2124</td></tr><tr><td>eval/samples_per_second</td><td>180.237</td></tr><tr><td>eval/steps_per_second</td><td>8.405</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/global_step</td><td>2370</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6685</td></tr><tr><td>train/total_flos</td><td>1.37133482053632e+16</td></tr><tr><td>train/train_loss</td><td>0.67426</td></tr><tr><td>train/train_runtime</td><td>738.2406</td></tr><tr><td>train/train_samples_per_second</td><td>70.6</td></tr><tr><td>train/train_steps_per_second</td><td>3.21</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 10 media file(s), 10 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">silvery-sweep-3</strong>: <a href=\"https://wandb.ai/capecape/aws_demo/runs/k24yo1h4\" target=\"_blank\">https://wandb.ai/capecape/aws_demo/runs/k24yo1h4</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220131_153713-k24yo1h4/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ulzkxomk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 57\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.454060753211388e-05\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/capecape/aws_demo/runs/ulzkxomk\" target=\"_blank\">lemon-sweep-4</a></strong> to <a href=\"https://wandb.ai/capecape/aws_demo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/capecape/aws_demo/sweeps/60gv5p2v\" target=\"_blank\">https://wandb.ai/capecape/aws_demo/sweeps/60gv5p2v</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/paperspace/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /home/paperspace/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running training *****\n",
      "  Num examples = 5212\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 57\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 57\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 368\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='368' max='368' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [368/368 04:22, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.443500</td>\n",
       "      <td>0.471727</td>\n",
       "      <td>0.737250</td>\n",
       "      <td>0.747946</td>\n",
       "      <td>0.778929</td>\n",
       "      <td>0.783961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.294100</td>\n",
       "      <td>0.418173</td>\n",
       "      <td>0.800969</td>\n",
       "      <td>0.791191</td>\n",
       "      <td>0.797927</td>\n",
       "      <td>0.787709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>0.536774</td>\n",
       "      <td>0.817460</td>\n",
       "      <td>0.814633</td>\n",
       "      <td>0.823834</td>\n",
       "      <td>0.812358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.708236</td>\n",
       "      <td>0.812326</td>\n",
       "      <td>0.815040</td>\n",
       "      <td>0.827288</td>\n",
       "      <td>0.818331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 57\n",
      "Saving model checkpoint to training_dir/checkpoint-92\n",
      "Configuration saved in training_dir/checkpoint-92/config.json\n",
      "Model weights saved in training_dir/checkpoint-92/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-92/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-92/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-2133] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 57\n",
      "Saving model checkpoint to training_dir/checkpoint-184\n",
      "Configuration saved in training_dir/checkpoint-184/config.json\n",
      "Model weights saved in training_dir/checkpoint-184/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-184/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-184/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-2370] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 57\n",
      "Saving model checkpoint to training_dir/checkpoint-276\n",
      "Configuration saved in training_dir/checkpoint-276/config.json\n",
      "Model weights saved in training_dir/checkpoint-276/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-276/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-276/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-92] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 57\n",
      "Saving model checkpoint to training_dir/checkpoint-368\n",
      "Configuration saved in training_dir/checkpoint-368/config.json\n",
      "Model weights saved in training_dir/checkpoint-368/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-368/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-368/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-184] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8142... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.09MB of 0.09MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▄▇█</td></tr><tr><td>eval/f1</td><td>▁▆██</td></tr><tr><td>eval/loss</td><td>▂▁▄█</td></tr><tr><td>eval/precision</td><td>▁▂▇█</td></tr><tr><td>eval/recall</td><td>▁▇██</td></tr><tr><td>eval/runtime</td><td>█▄▁▂</td></tr><tr><td>eval/samples_per_second</td><td>▁▅█▇</td></tr><tr><td>eval/steps_per_second</td><td>▁▅█▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▇█▇▇▇▆▆▆▅▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▂▂▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.82729</td></tr><tr><td>eval/f1</td><td>0.81504</td></tr><tr><td>eval/loss</td><td>0.70824</td></tr><tr><td>eval/precision</td><td>0.81833</td></tr><tr><td>eval/recall</td><td>0.81233</td></tr><tr><td>eval/runtime</td><td>3.021</td></tr><tr><td>eval/samples_per_second</td><td>191.655</td></tr><tr><td>eval/steps_per_second</td><td>3.641</td></tr><tr><td>train/epoch</td><td>4.0</td></tr><tr><td>train/global_step</td><td>368</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0344</td></tr><tr><td>train/total_flos</td><td>5485339282145280.0</td></tr><tr><td>train/train_loss</td><td>0.26882</td></tr><tr><td>train/train_runtime</td><td>263.6464</td></tr><tr><td>train/train_samples_per_second</td><td>79.076</td></tr><tr><td>train/train_steps_per_second</td><td>1.396</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 4 media file(s), 4 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">lemon-sweep-4</strong>: <a href=\"https://wandb.ai/capecape/aws_demo/runs/ulzkxomk\" target=\"_blank\">https://wandb.ai/capecape/aws_demo/runs/ulzkxomk</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220131_154941-ulzkxomk/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: unvkfmuk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0007055478708026065\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/capecape/aws_demo/runs/unvkfmuk\" target=\"_blank\">fearless-sweep-5</a></strong> to <a href=\"https://wandb.ai/capecape/aws_demo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/capecape/aws_demo/sweeps/60gv5p2v\" target=\"_blank\">https://wandb.ai/capecape/aws_demo/sweeps/60gv5p2v</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/paperspace/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /home/paperspace/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running training *****\n",
      "  Num examples = 5212\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 18\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 18\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1740\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1740' max='1740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1740/1740 07:29, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.679700</td>\n",
       "      <td>0.720638</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.702900</td>\n",
       "      <td>0.743546</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.275344</td>\n",
       "      <td>0.379965</td>\n",
       "      <td>0.189983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.656500</td>\n",
       "      <td>0.670645</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.693500</td>\n",
       "      <td>0.690597</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.666500</td>\n",
       "      <td>0.665170</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.702500</td>\n",
       "      <td>0.664235</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.620035</td>\n",
       "      <td>0.310017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to training_dir/checkpoint-290\n",
      "Configuration saved in training_dir/checkpoint-290/config.json\n",
      "Model weights saved in training_dir/checkpoint-290/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-290/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-290/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-276] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to training_dir/checkpoint-580\n",
      "Configuration saved in training_dir/checkpoint-580/config.json\n",
      "Model weights saved in training_dir/checkpoint-580/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-580/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-580/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-368] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to training_dir/checkpoint-870\n",
      "Configuration saved in training_dir/checkpoint-870/config.json\n",
      "Model weights saved in training_dir/checkpoint-870/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-870/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-870/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-290] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to training_dir/checkpoint-1160\n",
      "Configuration saved in training_dir/checkpoint-1160/config.json\n",
      "Model weights saved in training_dir/checkpoint-1160/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-1160/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-1160/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-580] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to training_dir/checkpoint-1450\n",
      "Configuration saved in training_dir/checkpoint-1450/config.json\n",
      "Model weights saved in training_dir/checkpoint-1450/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-1450/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-1450/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-870] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 579\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to training_dir/checkpoint-1740\n",
      "Configuration saved in training_dir/checkpoint-1740/config.json\n",
      "Model weights saved in training_dir/checkpoint-1740/pytorch_model.bin\n",
      "tokenizer config file saved in training_dir/checkpoint-1740/tokenizer_config.json\n",
      "Special tokens file saved in training_dir/checkpoint-1740/special_tokens_map.json\n",
      "Deleting older checkpoint [training_dir/checkpoint-1160] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8534... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.13MB of 0.13MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>█▁████</td></tr><tr><td>eval/f1</td><td>█▁████</td></tr><tr><td>eval/loss</td><td>▆█▂▃▁▁</td></tr><tr><td>eval/precision</td><td>█▁████</td></tr><tr><td>eval/recall</td><td>▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▃▁█▆▂</td></tr><tr><td>eval/samples_per_second</td><td>█▆█▁▃▆</td></tr><tr><td>eval/steps_per_second</td><td>█▆█▁▃▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▅▄▆▆▇▄▅▅▅▃▆▅▆▅▃▅▇▅▅▃▅▄▄▃▃▅▆▄▇▄▅▃▅▃▅█▁▄▅▅</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.62003</td></tr><tr><td>eval/f1</td><td>0.38273</td></tr><tr><td>eval/loss</td><td>0.66423</td></tr><tr><td>eval/precision</td><td>0.31002</td></tr><tr><td>eval/recall</td><td>0.5</td></tr><tr><td>eval/runtime</td><td>3.1648</td></tr><tr><td>eval/samples_per_second</td><td>182.951</td></tr><tr><td>eval/steps_per_second</td><td>10.427</td></tr><tr><td>train/epoch</td><td>6.0</td></tr><tr><td>train/global_step</td><td>1740</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.7025</td></tr><tr><td>train/total_flos</td><td>8228008923217920.0</td></tr><tr><td>train/train_loss</td><td>0.67564</td></tr><tr><td>train/train_runtime</td><td>449.9093</td></tr><tr><td>train/train_samples_per_second</td><td>69.507</td></tr><tr><td>train/train_steps_per_second</td><td>3.867</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 6 media file(s), 6 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fearless-sweep-5</strong>: <a href=\"https://wandb.ai/capecape/aws_demo/runs/unvkfmuk\" target=\"_blank\">https://wandb.ai/capecape/aws_demo/runs/unvkfmuk</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220131_155418-unvkfmuk/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train_sweep, count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d65d79-c659-4b07-9193-0ffa750167e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
